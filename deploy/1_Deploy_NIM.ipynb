{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e60cb36-f153-4991-a31f-702f11144446",
   "metadata": {},
   "source": [
    "# Deploy Any LLM with NIM\n",
    "\n",
    "This notebook demonstrates how to deploy almost any Large Language Model (LLMs) using NVIDIA NIM. NIM provides a streamlined way to deploy and serve LLMs with optimized performance and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f2acb-52d9-49dd-9676-2d58715786f5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deploying various LLMs often involves working with multiple inference frameworks and manual optimization, which can be time-consuming. NIM simplifies this by providing a consistent interface and automatically handling model analysis, backend selection, and configuration.\n",
    "\n",
    "This tutorial covers:\n",
    "*   Understanding how NIM handles different model formats.\n",
    "*   Deploying models directly from Hugging Face.\n",
    "*   Listing available backend options for a model.\n",
    "*   Deploying models from local storage.\n",
    "*   Customizing deployments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c7758-746f-4e00-8a28-cbdc463a7925",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    ">[Prerequisites](#Prerequisites)  \n",
    ">[Spin Up Blueprint](#Spin-Up-Blueprint)  \n",
    ">[Download Sample Data](#Download-Sample-Data)  \n",
    ">[Validate Deployment](#Validate-Deployment)  \n",
    ">[API Reference](#API-Reference)  \n",
    ">[Next Steps](#Next-Steps)  \n",
    ">[Shutting Down Blueprint](#Stopping-Services-and-Cleaning-Up)  \n",
    ">[Appendix](#Appendix)  \n",
    "________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fcc47-fb41-4e54-9d30-4d17bc483779",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09678c9-8fbe-41d7-84ad-ce624bec582c",
   "metadata": {},
   "source": [
    "### Clone repository and install software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ded68-437d-4ad2-be82-b36ff6100b30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1. **Clone** <name> Git repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197704e-b63c-42fc-be4b-4f3fb03acfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone ssh://git@github.com:NVIDIA-AI-Blueprints/Universal-LLM-NIM.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2b89e-f8c3-458e-88a4-ecfac63a9916",
   "metadata": {},
   "source": [
    "2. Install **[Docker](https://docs.docker.com/engine/install/ubuntu/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8d198-db10-4529-b76b-14f0c5cf216d",
   "metadata": {},
   "source": [
    "3. Install **[NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-the-nvidia-container-toolkit)** to configure Docker for GPU-accelerated containers, like NVIDIA NIM.\n",
    " If you are using a system deployed with Brev you can skip this step since Brev systems come with NVIDIA Container Toolkit preinstalled. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b33bb-eade-43ec-806a-1d1dcdc7e773",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> After installing the toolkit, follow the instructions in the Configure Docker section in the NVIDIA Container Toolkit documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26d7bf-0044-45c4-bace-18b63c41dd04",
   "metadata": {},
   "source": [
    "### Get a API Keys\n",
    "\n",
    "#### Let's start by logging into the NVIDIA Container Registry. \n",
    " \n",
    "The NVIDIA NGC API Key is a mandatory key that is required to use this blueprint. This is needed to log into the NVIDIA container registry, nvcr.io, and to pull secure container images used in this NVIDIA NIM Blueprint.\n",
    "Refer to [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key) in the NVIDIA NGC User Guide for more information.\n",
    "\n",
    "\n",
    "\n",
    "Authenticate with the NVIDIA Container Registry with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe4bca-f542-4610-8424-0b97a57237db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker login nvcr.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441d659-be45-422e-a732-530750286da7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Use oauthtoken as the username and your API key as the password. The $oauthtoken username is a special name that indicates that you will authenticate with an API key and not a user name and password.After installing the toolkit, follow the instructions in the Configure Docker section in the NVIDIA Container Toolkit documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edd67a-83b0-48fb-99bd-589c484cc761",
   "metadata": {},
   "source": [
    "You'll also need a [Huggingface Token](https://huggingface.co/settings/tokens) to download some of the models in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc799586-6a85-4f18-bc3f-f3a778509606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    hf_token = getpass.getpass(\"Enter your Huggingface Token: \")\n",
    "    assert hf_token.startswith(\"hf_\"), \"Not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7f3c4-d8c3-4b96-bb43-5c15c4c4918b",
   "metadata": {},
   "source": [
    "## Deployment Examples\n",
    "\n",
    "Let's explore different ways to deploy models using NIM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e8f732",
   "metadata": {},
   "source": [
    "### Example 1: Basic Model Deployment from Hugging Face or Local Filesystem\n",
    "\n",
    "This example shows how to deploy Codestral-22B, a powerful code generation model, directly from Hugging Face. Note that you need to accept the model's access agreement before you can use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying directly from Hugging Face\n",
    "!docker run --rm --gpus all \\\n",
    "  --network=host \\\n",
    "  -u $(id -u) \\\n",
    "  -v $(pwd)/nim_cache:/opt/nim/.cache \\\n",
    "  -v $(pwd):$(pwd) \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  -e NIM_TENSOR_PARALLEL_SIZE=1 \\\n",
    "  $NIM_IMAGE nim-run --model \"hf://mistralai/Codestral-22B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf3737",
   "metadata": {},
   "source": [
    "If your model is already downloaded locally, you can simply point nim-run to where it exists on your filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb073ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying a model that is available on the local filesystem\n",
    "!docker run --rm --gpus all \\\n",
    "  --network=host \\\n",
    "  -u $(id -u) \\\n",
    "  -v $(pwd)/nim_cache:/opt/nim/.cache \\\n",
    "  -v $(pwd):$(pwd) \\\n",
    "  -v /path/to/model/dir:/path/to/model/dir \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  -e NIM_TENSOR_PARALLEL_SIZE=1 \\\n",
    "  $NIM_IMAGE nim-run --model \"/path/to/model/dir/mistralai-Codestral-22B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc45fc7",
   "metadata": {},
   "source": [
    "Once the model is downloaded or loaded from the local filesystem, NIM recognizes it as a full-precision Mistral model, selects the optimal backend (typically TensorRT-LLM for best performance), and configures the server optimally for your hardware. Additionally, we are specifying tensor parallelism to be 1, which you can change if you intend to use multiple GPUs for deployment. You can inspect the full list of supported arguments by running `nim-run --help` in the container. The deployed model will be available at http://localhost:8000 for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdc0be",
   "metadata": {},
   "source": [
    "### Example 2: Exploring Available Backend Options\n",
    "\n",
    "NIM supports multiple backends for model deployment. Let's see how to specify different backends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b79866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TensorRT-LLM backend\n",
    "!docker run --rm --gpus all \\\n",
    "  --network=host \\\n",
    "  -u $(id -u) \\\n",
    "  -v $(pwd)/nim_cache:/opt/nim/.cache \\\n",
    "  -v $(pwd):$(pwd) \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  -e NIM_TENSOR_PARALLEL_SIZE=1 \\\n",
    "  $NIM_IMAGE nim-run --model \"hf://mistralai/Codestral-22B-v0.1\" --backend tensorrt-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28714a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using vLLM backend\n",
    "!docker run --rm --gpus all \\\n",
    "  --network=host \\\n",
    "  -u $(id -u) \\\n",
    "  -v $(pwd)/nim_cache:/opt/nim/.cache \\\n",
    "  -v $(pwd):$(pwd) \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  -e NIM_TENSOR_PARALLEL_SIZE=1 \\\n",
    "  $NIM_IMAGE nim-run --model \"hf://mistralai/Codestral-22B-v0.1\" --backend vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993e434",
   "metadata": {},
   "source": [
    "### Example 3: Customizing Model Parameters\n",
    "\n",
    "You can customize various model parameters to optimize performance and resource usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a99663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying with custom parameters\n",
    "!docker run --rm --gpus all \\\n",
    "  --network=host \\\n",
    "  -u $(id -u) \\\n",
    "  -v $(pwd)/nim_cache:/opt/nim/.cache \\\n",
    "  -v $(pwd):$(pwd) \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  -e NIM_TENSOR_PARALLEL_SIZE=1 \\\n",
    "  -e NIM_MAX_BATCH_SIZE=32 \\\n",
    "  -e NIM_MAX_INPUT_LENGTH=2048 \\\n",
    "  -e NIM_MAX_OUTPUT_LENGTH=512 \\\n",
    "  $NIM_IMAGE nim-run --model \"hf://mistralai/Codestral-22B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942805f-f317-48f8-9d74-51e125ba50b4",
   "metadata": {},
   "source": [
    "## Using the Deployed Model\n",
    "\n",
    "Once your model is deployed, you can interact with it using the REST API. Here's an example of how to make requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def generate_text(prompt, max_tokens=100):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/v1/completions\",\n",
    "        json={\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Example usage\n",
    "result = generate_text(\"Write a Python function to calculate fibonacci numbers:\")\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d774fc0-3b2c-4482-83c4-cbfa32851eae",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "For detailed API references, please refer to the following locations in the Blueprint repository:\n",
    "- Summary & Conversation APIs:\n",
    "`./docs/api_references/analytics_server.json`\n",
    "\n",
    "- Generate API:\n",
    "`./docs/api_references/agent_server.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24f9a7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "NIM significantly simplifies deploying a wide variety of LLMs by automating model analysis, backend selection, and optimization. It provides a consistent and efficient workflow for AI builders, enabling rapid experimentation and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23cdd4d-2f22-46c1-ab94-99847c1c7dbb",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "*   [NIM documentation](https://docs.nvidia.com/nim/)\n",
    "*   [Supported model architectures](https://docs.nvidia.com/nim/supported-models)\n",
    "*   [Backend selection details](https://docs.nvidia.com/nim/backends)\n",
    "*   [NVIDIA AI forums](https://forums.developer.nvidia.com/c/ai-deep-learning/nemo-and-generative-ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
