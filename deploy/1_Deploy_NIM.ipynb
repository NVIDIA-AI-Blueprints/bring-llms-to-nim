{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d92c65",
   "metadata": {},
   "source": [
    "<!--\n",
    "SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "-->\n",
    "\n",
    "# Deploy Any LLM with NIM\n",
    "\n",
    "This notebook demonstrates how to deploy almost any Large Language Model (LLMs) using NVIDIA NIM. NIM provides a streamlined way to deploy and serve LLMs with optimized performance and flexibility.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Deploying various LLMs often involves working with multiple inference frameworks and manual optimization, which can be time-consuming. NIM simplifies this by providing a consistent interface and automatically handling model analysis, backend selection, and configuration.\n",
    "\n",
    "This tutorial covers:\n",
    "*   Understanding how NIM handles different model formats.\n",
    "*   Deploying models directly from Hugging Face.\n",
    "*   Listing available backend options for a model.\n",
    "*   Customizing deployments.\n",
    "*   Deploying models from local storage.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "Before proceeding, ensure your system meets the following requirements:\n",
    "\n",
    "- **GPU**: NVIDIA GPU with at least 24GB VRAM (for Codestral-22B) or 8GB VRAM (for smaller models like Qwen2.5-0.5B)\n",
    "- **Driver**: NVIDIA Driver version 535 or higher\n",
    "- **CUDA**: CUDA 12.0 or higher\n",
    "- **System Memory**: At least 32GB RAM recommended\n",
    "- **Storage**: Sufficient disk space for model downloads and caching\n",
    "\n",
    "For detailed hardware specifications, please refer to the [NIM LLM Documentation](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html).\n",
    "\n",
    "### System Setup\n",
    "\n",
    "First, let's verify your GPU setup and install necessary dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90a4c6",
   "metadata": {},
   "source": [
    "### Install Required Software\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fcb5ca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Update system and install git-lfs for model downloads\n",
    "!sudo apt-get update && echo \"✓ System packages updated successfully\"\n",
    "!sudo apt-get install git-lfs -y && echo \"✓ Git LFS installed successfully\"\n",
    "!git lfs install && echo \"✓ Git LFS initialized successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies for Docker management\n",
    "!pip install docker requests && echo \"✓ Python Docker SDK and requests installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34ca64",
   "metadata": {},
   "source": [
    "### Get API Keys\n",
    "\n",
    "#### NVIDIA NGC API Key\n",
    "\n",
    "The NVIDIA NGC API Key is mandatory for accessing NVIDIA container registry and pulling secure container images.\n",
    "Refer to [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eff1707",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"NGC_API_KEY\"] = \"YOUR_NGC_API_KEY_HERE\"\n",
    "\n",
    "if not os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    ngc_api_key = getpass.getpass(\"Enter your NGC API Key: \")\n",
    "    assert ngc_api_key.startswith(\"nvapi-\"), \"Not a valid key\"\n",
    "    os.environ[\"NGC_API_KEY\"] = ngc_api_key\n",
    "    print(\"✓ NGC API Key set successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eefbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06360d08",
   "metadata": {},
   "source": [
    "#### Hugging Face Token\n",
    "\n",
    "You'll also need a [Huggingface Token](https://huggingface.co/settings/tokens) to download models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37843e43",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if not os.environ.get(\"HF_USERNAME\", \"\"):\n",
    "    # hf_username = getpass.getpass(\"Enter your Huggingface Username: \")\n",
    "    os.environ[\"HF_USERNAME\"] = \"nealv\"\n",
    "    print(\"✓ Hugging Face username set successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2978e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    # hf_token = getpass.getpass(\"Enter your Huggingface Token: \")\n",
    "    # assert hf_token.startswith(\"hf_\"), \"Not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN_HERE\"\n",
    "    print(\"✓ Hugging Face token set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31df86",
   "metadata": {},
   "source": [
    "### Setup NIM Container\n",
    "\n",
    "<!-- FIX THIS BIT -->\n",
    "Choose your NIM container image and pull it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6728f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Available NIM container options (choose one):\n",
    "# nim_images = {\n",
    "#     \"universal\": \"nvcr.io/nvidian/nim-llm-dev/universal-nim:1.11.0.rc4\",\n",
    "#     \"latest\": \"nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\"\n",
    "# }\n",
    "\n",
    "# Set the NIM image - you can change this to your preferred version\n",
    "os.environ['NIM_IMAGE'] = \"nvcr.io/nvidian/nim-llm-dev/universal-nim:1.11.0.rc6\"\n",
    "print(f\"Using NIM image: {os.environ['NIM_IMAGE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43883941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the NIM container image\n",
    "!docker pull $NIM_IMAGE\n",
    "print(\"✓ NIM container image pulled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956bb999",
   "metadata": {},
   "source": [
    "### Download Model to Local Storage\n",
    "\n",
    "We'll download Qwen2.5-0.5B, a lightweight LLM, for use in Example 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1863354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/models/Qwen2.5-0.5B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e27f927",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b>  NVIDIA cannot guarantee the security of any models hosted on non-NVIDIA systems such as HuggingFace. Malicious or insecure models can result in serious security risks up to and including full remote code execution. We strongly recommend that before attempting to load it you manually verify the safety of any model not provided by NVIDIA, through such mechanisms as a) ensuring that the model weights are serialized using the safetensors format, b) conducting a manual review of any model or inference code to ensure that it is free of obfuscated or malicious code, and c) validating the signature of the model, if available, to ensure that it comes from a trusted source and has not been modified.\n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22dadf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://$HF_USERNAME:$HF_TOKEN@huggingface.co/Qwen/Qwen2.5-0.5B \\\n",
    "    ~/models/Qwen2.5-0.5B && echo \"✓ Qwen2.5-0.5B model downloaded successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95ad2b",
   "metadata": {},
   "source": [
    "## Deployment Examples\n",
    "\n",
    "Let's explore different ways to deploy models using NIM.\n",
    "\n",
    "### Example 1: Basic Deployment from Hugging Face\n",
    "\n",
    "This example shows how to deploy Codestral-22B directly from Hugging Face.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b> You must accept the model's license agreement at https://huggingface.co/mistralai/Codestral-22B-v0.1 before using this model.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b8fc4c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "os.environ[\"CONTAINER_NAME\"] = \"LLM-NIM\"\n",
    "# os.environ['NIM_IMAGE'] = \"...\" # TODO: Need to change to public URL\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = os.path.expanduser(\"~/.cache/nim\")\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce19df",
   "metadata": {},
   "source": [
    "After running the following cell, you should be able to see the `LLM-NIM` container running.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps  # Check container is running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8db30",
   "metadata": {},
   "source": [
    "While the LLM NIM service is getting ready, you may run the following cell to see live logs.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note:</b> NIM service takes a few minutes to initialize. Monitor with logs if needed.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a6ac9ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Optional: Monitor logs during startup (set print_logs=True to see detailed logs)\n",
    "# check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4905274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import docker\n",
    "import os\n",
    "\n",
    "def check_service_ready_from_logs(container_name, print_logs=False, timeout=600):\n",
    "    \"\"\"\n",
    "    Check if NIM service is ready by monitoring Docker logs for 'Application startup complete' message.\n",
    "\n",
    "    Args:\n",
    "        container_name (str): Name of the Docker container\n",
    "        print_logs (bool): Whether to print logs while monitoring (default: False)\n",
    "        timeout (int): Maximum time to wait in seconds (default: 600)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if service is ready, False if timeout reached\n",
    "    \"\"\"\n",
    "    print(\"Waiting for NIM service to start...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        client = docker.from_env()\n",
    "        container = client.containers.get(container_name)\n",
    "\n",
    "                # Stream logs in real-time using the blocking generator\n",
    "        log_buffer = \"\"\n",
    "        for log_chunk in container.logs(stdout=True, stderr=True, follow=True, stream=True):\n",
    "            # Check timeout\n",
    "            if time.time() - start_time > timeout:\n",
    "                print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "                return False\n",
    "\n",
    "            # Decode chunk and add to buffer\n",
    "            chunk = log_chunk.decode('utf-8', errors='ignore')\n",
    "            log_buffer += chunk\n",
    "\n",
    "            # Process complete lines\n",
    "            while '\\n' in log_buffer:\n",
    "                line, log_buffer = log_buffer.split('\\n', 1)\n",
    "                line = line.strip()\n",
    "\n",
    "                if print_logs and line:\n",
    "                    print(f\"[LOG] {line}\")\n",
    "\n",
    "                # Check for startup complete message\n",
    "                if \"Application startup complete\" in line:\n",
    "                    print(\"✓ Application startup complete! Service is ready.\")\n",
    "                    return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "    return False\n",
    "\n",
    "def check_service_ready():\n",
    "    \"\"\"Fallback health check using HTTP endpoint\"\"\"\n",
    "    url = 'http://localhost:8000/v1/health/ready'\n",
    "    print(\"Checking service health endpoint...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers={'accept': 'application/json'})\n",
    "            if response.status_code == 200 and response.json().get(\"message\") == \"Service is ready.\":\n",
    "                print(\"✓ Service ready!\")\n",
    "                break\n",
    "        except requests.ConnectionError:\n",
    "            pass\n",
    "        print(\"⏳ Still starting...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "# Use the log-based check first, fallback to health endpoint if needed\n",
    "container_name = os.environ.get(\"CONTAINER_NAME\", \"LLM-NIM\")\n",
    "if not check_service_ready_from_logs(container_name, print_logs=True):\n",
    "    print(\"Falling back to health endpoint check...\")\n",
    "    check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a1f7b",
   "metadata": {},
   "source": [
    "Now let's test the deployed model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeda93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def generate_text(model, prompt, max_tokens=1000, temperature=0.7):\n",
    "    \"\"\"Generate text using the NIM service\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"http://localhost:8000/v1/completions\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "result = generate_text(\n",
    "    model=\"mistralai/Codestral-22B-v0.1\",\n",
    "    prompt=\"Write a complete function that computes fibonacci numbers in Rust:\"\n",
    ")\n",
    "print(\"Generated Code:\")\n",
    "print(\"=\" * 50)\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7215a241",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e7986",
   "metadata": {},
   "source": [
    "### Example 2: Deployment Using Different Backend Options\n",
    "\n",
    "NIM supports multiple backends for model deployment. Let's explore TensorRT-LLM and vLLM backends:\n",
    "\n",
    "#### TensorRT-LLM Backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6011b9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Using TensorRT-LLM backend by specifying the NIM_MODEL_PROFILE parameter\n",
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_MODEL_PROFILE=\"tensorrt_llm\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae2259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "if not check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True):\n",
    "    print(\"Falling back to health endpoint check...\")\n",
    "    check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdeacf9",
   "metadata": {},
   "source": [
    "Test the TensorRT-LLM backend:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\n",
    "    model=\"mistralai/Codestral-22B-v0.1\",\n",
    "    prompt=\"Write a complete Python function that computes fibonacci numbers with memoization:\"\n",
    ")\n",
    "print(\"TensorRT-LLM Backend Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b699754",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a770610",
   "metadata": {},
   "source": [
    "#### vLLM Backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f834877",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Using vLLM backend by specifying the NIM_MODEL_PROFILE parameter\n",
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_MODEL_PROFILE=\"vllm\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534fe538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "if not check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True):\n",
    "    print(\"Falling back to health endpoint check...\")\n",
    "    check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e3670",
   "metadata": {},
   "source": [
    "Test the vLLM backend:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\n",
    "    model=\"mistralai/Codestral-22B-v0.1\",\n",
    "    prompt=\"Write a complete C++ function that computes fibonacci numbers efficiently:\"\n",
    ")\n",
    "print(\"vLLM Backend Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ffd1d",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57ea02",
   "metadata": {},
   "source": [
    "### Example 3: Customizing Model Parameters\n",
    "\n",
    "This example demonstrates how custom parameters affect model behavior. We'll deploy with specific constraints and test them:\n",
    "\n",
    "**Key Parameters:**\n",
    "* `NIM_TENSOR_PARALLEL_SIZE=2`: Uses 2 GPUs in parallel for better performance\n",
    "* `NIM_MAX_INPUT_LENGTH=2048`: Limits input to 2048 tokens\n",
    "* `NIM_MAX_OUTPUT_LENGTH=512`: Limits output to 512 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae008cc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_TENSOR_PARALLEL_SIZE=2 \\\n",
    " -e NIM_MAX_INPUT_LENGTH=2048 \\\n",
    " -e NIM_MAX_OUTPUT_LENGTH=512 \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "if not check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True):\n",
    "    print(\"Falling back to health endpoint check...\")\n",
    "    check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5900ab",
   "metadata": {},
   "source": [
    "Test with custom parameters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(model=\"mistralai/Codestral-22B-v0.1\",\n",
    "                       prompt=\"Write me a function that computes fibonacci in Javascript\")\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6eb5b",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234309d",
   "metadata": {},
   "source": [
    "### Example 4: Deployment from Local Model\n",
    "\n",
    "This example shows how to deploy Qwen2.5-0.5B from the locally downloaded model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58d804",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Verify model files exist\n",
    "!ls ~/models/Qwen2.5-0.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e32e81b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "os.environ[\"LOCAL_MODEL_DIR\"] = os.path.expanduser(\"~/models/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99c661",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus '\"device=0\"' \\\n",
    " --shm-size=16GB \\\n",
    " -e NIM_MODEL_NAME=\"/opt/models/Qwen2.5-0.5B\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"Qwen/Qwen2.5-0.5B\" \\\n",
    " -v \"$LOCAL_MODEL_DIR:/opt/models/Qwen2.5-0.5B\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "if not check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True):\n",
    "    print(\"Falling back to health endpoint check...\")\n",
    "    check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247ffef",
   "metadata": {},
   "source": [
    "Test the local model deployment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c99a11",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "result = generate_text(model=\"Qwen/Qwen2.5-0.5B\",\n",
    "                       prompt=\"Once upon a time \")\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "!docker stop $CONTAINER_NAME\n",
    "print(\"✓ All containers stopped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06e635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
