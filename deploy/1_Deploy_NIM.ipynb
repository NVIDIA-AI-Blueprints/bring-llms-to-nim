{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5b7fa8-2330-4dae-a8ef-4ec8f2acb253",
   "metadata": {},
   "source": [
    "# Deploy Any LLM with NIM\n",
    "\n",
    "This notebook demonstrates how to deploy almost any Large Language Model (LLMs) using NVIDIA NIM. NIM provides a streamlined way to deploy and serve LLMs with optimized performance and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb8cca-f40c-42de-9e38-d9de9390b425",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deploying various LLMs often involves working with multiple inference frameworks and manual optimization, which can be time-consuming. NIM simplifies this by providing a consistent interface and automatically handling model analysis, backend selection, and configuration.\n",
    "\n",
    "This tutorial covers:\n",
    "*   Understanding how NIM handles different model formats.\n",
    "*   Deploying models directly from Hugging Face.\n",
    "*   Listing available backend options for a model.\n",
    "*   Customizing deployments.\n",
    "*   Deploying models from local storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44df0f-ea3e-4103-b78e-caa886026cfa",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da7892-a029-416f-b7b6-ed24a4aafe11",
   "metadata": {},
   "source": [
    "### Clone repository and install software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99a35c-16ee-44c4-8a8e-b46280e08bb5",
   "metadata": {},
   "source": [
    "1. **Clone** <name> Git repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44abedf-1785-4fa1-a050-8d0bbe9a657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone ssh://git@github.com:NVIDIA-AI-Blueprints/Universal-LLM-NIM.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4beed59-11f4-4126-8600-cf25c8b0c6e3",
   "metadata": {},
   "source": [
    "2. Verify the Driver and CUDA version to be the following:\n",
    "- Driver Version: 535.x.x\n",
    "- CUDA Version 12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a107500-22c4-40ec-b0b2-0d02895e9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995bd6a3-cf2e-4d4d-923e-293ec366a531",
   "metadata": {},
   "source": [
    "If the driver version doesn't match in the above step:\n",
    "- Update the Driver to 535\n",
    "- Reboot the system\n",
    "- Set ```NGC_API_KEY``` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41d6a5-6614-4d1c-9866-888cda3d9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt install nvidia-driver-535 -y\n",
    "# !sudo reboot now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75b023-bc87-4d6a-ade3-16c460d8a98a",
   "metadata": {},
   "source": [
    "### Get a API Keys\n",
    "\n",
    "#### Let's start by logging into the NVIDIA Container Registry. \n",
    " \n",
    "The NVIDIA NGC API Key is a mandatory key that is required to use this blueprint. This is needed to log into the NVIDIA container registry, nvcr.io, and to pull secure container images used in this NVIDIA NIM Blueprint.\n",
    "Refer to [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key) in the NVIDIA NGC User Guide for more information.\n",
    "\n",
    "\n",
    "\n",
    "Authenticate with the NVIDIA Container Registry with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12375715-a15c-4d15-b20b-b52f1a6fa9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NGC_API_KEY\"] = \"*****\" # Replace with your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6549e4c-c67c-4b23-a004-8c35320b070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8fedb-5d7d-49d8-a5c3-56758273d138",
   "metadata": {},
   "source": [
    "You'll also need a [Huggingface Token](https://huggingface.co/settings/tokens) to download the models in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf19626-e495-487e-8c79-5154cb174e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_USERNAME\"] = \"*****\" # Replace with your HuggingFace username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a479ef2-1544-4a87-a5d2-7a89a42ba050",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    hf_token = getpass.getpass(\"Enter your Huggingface Token: \")\n",
    "    assert hf_token.startswith(\"hf_\"), \"Not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ccf617-b84d-4db5-aaff-5fe947edd83b",
   "metadata": {},
   "source": [
    "Updating the docker storage path to Ephemeral storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9dbf9f-0cba-43d2-8627-3ddaf721aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, subprocess, time\n",
    "\n",
    "storage_path = \"/ephemeral/cache/docker\"\n",
    "\n",
    "daemon_file = \"/etc/docker/daemon.json\" #update the path if required\n",
    "config = {}\n",
    "try:\n",
    "    config = json.load(open(daemon_file)) if os.path.exists(daemon_file) else {}\n",
    "except PermissionError:\n",
    "    print(\"Cannot read the file. Try running with elevated privileges or check docker deamon file path.\")\n",
    "\n",
    "config[\"data-root\"] = storage_path\n",
    "config_str = json.dumps(config, indent=4)\n",
    "\n",
    "subprocess.run(f\"echo '{config_str}' | sudo tee {daemon_file} > /dev/null\", shell=True, check=True)\n",
    "subprocess.run(\"sudo systemctl restart docker\", shell=True, check=True)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Verify new storage location\n",
    "print(subprocess.run(\"docker info | grep 'Docker Root Dir'\", shell=True, capture_output=True, text=True).stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aca665-c04e-4fe2-a0a7-1a39e13a66f1",
   "metadata": {},
   "source": [
    "### Downloading model to local storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593944fb-0e79-4d0b-be8c-1dcbd7d6a63e",
   "metadata": {},
   "source": [
    "You will use Qwen2.5-0.5B, a lightweight LLM, later in Example 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c141c1e-3484-4231-bace-807f37e7c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /ephemeral/models/Qwen2.5-0.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3585b-cbc4-4789-9218-f0133739462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://$HF_USERNAME:$HF_TOKEN@huggingface.co/Qwen/Qwen2.5-0.5B \\\n",
    "    /ephemeral/models/Qwen2.5-0.5B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d1d95-aa3a-42a6-ae6e-7ecb38923ad5",
   "metadata": {},
   "source": [
    "## Deployment Examples\n",
    "\n",
    "Let's explore different ways to deploy models using NIM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66d272-a1ed-40af-9681-4cb29f4bcede",
   "metadata": {},
   "source": [
    "### Example 1: Basic Deployment from Hugging Face\n",
    "\n",
    "This example shows how to deploy Codestral-22B, a powerful code generation model, directly from Hugging Face. Note that you need to accept the model's access agreement before you can use this model. To accept the agreement, you may visit https://huggingface.co/mistralai/Codestral-22B-v0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R $(whoami) /ephemeral/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b1a34-7435-457b-b02a-8f45ebb211c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONTAINER_NAME\"] = \"Universal-LLM-NIM\"\n",
    "os.environ['NIM_IMAGE'] = \"***\" # TODO: Need to change to public URL\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = os.path.expanduser(\"/ephemeral/cache/nim\")\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d912c-db46-43d0-bab9-93515b8c9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10632bdb-2388-40df-ae75-ff591473738d",
   "metadata": {},
   "source": [
    "After running the following cell, you should be able to see the `Universal-LLM-NIM` container running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17485588-e872-4e85-aaef-00fd61c6b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f557064-0105-4497-9129-56095e8b1063",
   "metadata": {},
   "source": [
    "While the LLM NIM service is getting ready, you may run the following cell to see live logs.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  LLM NIM service could take several miniutes to pull the model from Hugging Face and to get ready.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafaadd9-982d-4490-81f2-c0623bceed54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Uncomment the entire cell to see live logs if interested. Manually stop the cell once LLM NIM service is ready.\n",
    "# !docker logs -f $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea12a38-8fe0-47c3-840b-d8ac71f65df8",
   "metadata": {},
   "source": [
    "Below cell ensures that the LLM NIM is running before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da45fe0-3dd6-420b-851b-539cc7c12993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_service_ready():\n",
    "    url = 'http://localhost:8000/v1/health/ready'  # make sure the LLM NIM port is correct\n",
    "    headers = {'accept': 'application/json'}\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200 and response.json().get(\"message\") == \"Service is ready.\":\n",
    "                print(\"Service is ready.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Service is not ready. Waiting for 30 seconds...\")\n",
    "        except requests.ConnectionError:\n",
    "            print(\"Service is not ready. Waiting for 30 seconds...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820458af-2ac7-4268-8ac3-c955400e2767",
   "metadata": {},
   "source": [
    "Once your model is deployed, you can interact with it using the REST API. Here's an example of how to make requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f01e73-d3a9-4efe-8d84-bdcf1be0ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def generate_text(model, prompt, max_tokens=250):\n",
    "    response = requests.post(\n",
    "        f\"http://localhost:8000/v1/completions\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt, \n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Example usage\n",
    "result = generate_text(model=\"mistralai/Codestral-22B-v0.1\",\n",
    "                       prompt=\"Write me a function that computes fibonacci in Rust\")\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9946a4d0-cdc8-46d2-97d7-21223c577a66",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba1e27-9d27-4241-bb3a-06737fc14f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9942888-f696-4500-bbbf-3719d1a41f61",
   "metadata": {},
   "source": [
    "### Example 2: Deployment Using Available Backend Options\n",
    "\n",
    "NIM supports multiple backends for model deployment. Let's see how to specify different backends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ae89e-158a-43c6-a72a-90d364028362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TensorRT-LLM backend by specifying the NIM_MODEL_PROFILE parameter\n",
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_MODEL_PROFILE=\"tensorrt_llm\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad784e-9bdf-4746-a48f-fe7bc84e083d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # For live logs\n",
    "# !docker logs -f $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67effec-c0b5-4257-8242-ae5cd9d2765d",
   "metadata": {},
   "source": [
    "Below cell ensures that the LLM NIM is running before proceeding.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  LLM NIM service could take several miniutes to get ready.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57949caa-235b-4027-969c-d6824b1b2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c49e7-3115-4b89-a613-9d58723bede3",
   "metadata": {},
   "source": [
    "Let's try out the LLM NIM service backed by TRT-LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d54e1-0e2e-41b7-93ba-1ab5a60378a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = generate_text(model=\"mistralai/Codestral-22B-v0.1\",\n",
    "                       prompt=\"Write me a function that computes fibonacci in Python\")\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c0d50-90ec-4f2f-8caa-167aa51b1068",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e50d7-503a-45c9-a130-537c52270d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f2f1e-3f02-4346-825b-13485271d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using vLLM backend by specifying the NIM_MODEL_PROFILE parameter\n",
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_MODEL_PROFILE=\"vllm\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d92d74-63d1-43fb-a8ef-1892c528470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For live logs\n",
    "# !docker logs -f $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd69d9-04cf-4f6a-9b8b-2b861bc9af98",
   "metadata": {},
   "source": [
    "Below cell ensures that the LLM NIM is running before proceeding.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  LLM NIM service could take several miniutes to get ready.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb5a0a-35a7-49e7-9711-a96d0d2087bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea2d47-18cf-4e1f-b717-f7dbda657611",
   "metadata": {},
   "source": [
    "Let's try out the LLM NIM service backed by TRT-LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025b49c-bdcb-4ab0-b374-03dfb0b0acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(model=\"mistralai/Codestral-22B-v0.1\",\n",
    "                       prompt=\"Write me a function that computes fibonacci in C++\")\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b0b18-c1ae-4f2c-abca-73eca3b6d127",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e035f-d0d1-4e06-a058-33a071997cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6735863f-f882-4413-9587-977c13a4a1c9",
   "metadata": {},
   "source": [
    "### Example 3: Customizing Model Parameters\n",
    "\n",
    "You can customize various model parameters to optimize performance and resource usage. Here are some common parameters you might adjust:\n",
    "\n",
    "* `NIM_TENSOR_PARALLEL_SIZE`: Number of tensor parallel size to use. Increasing this can improve performance but will require more GPU memory.\n",
    "* `NIM_MAX_BATCH_SIZE`: Maximum number of samples to process in a single batch. Larger batch sizes can improve throughput but will also require more memory.\n",
    "* `NIM_MAX_INPUT_LENGTH`: Maximum length of input sequences. Adjusting this can help manage memory usage and processing time, especially for very long inputs.\n",
    "* `NIM_MAX_OUTPUT_LENGTH`: Maximum length of output sequences. This helps control the length of generated outputs, which can be important for tasks like text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee2f3f-3031-4d98-a8db-a298801a048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_TENSOR_PARALLEL_SIZE=2 \\\n",
    " -e NIM_MAX_BATCH_SIZE=16 \\\n",
    " -e NIM_MAX_INPUT_LENGTH=2048 \\\n",
    " -e NIM_MAX_OUTPUT_LENGTH=512 \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd749436-591e-41f4-a088-144d9f871935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For live logs\n",
    "!docker logs -f $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be66ee-ed10-446b-8216-90a4b0faf3e8",
   "metadata": {},
   "source": [
    "Below cell ensures that the LLM NIM is running before proceeding.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  LLM NIM service could take several miniutes to get ready.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af369990-2bcc-4222-a410-e1fdaa0e856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f41c9-0ffd-40fc-83da-ca6953f8d365",
   "metadata": {},
   "source": [
    "Let's try out the LLM NIM service with custom parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365bc05-465b-46e8-a277-6e90e374f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(model=\"mistralai/Codestral-22B-v0.1\",\n",
    "                       prompt=\"Write me a function that computes fibonacci in Javascript\")\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607b44c-c2d5-40a7-b25e-f4c2e016ee63",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee07c6-db1c-437b-a244-d05d2b85171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e833a-b334-42fd-bbae-dea1aa9bfc29",
   "metadata": {},
   "source": [
    "### Example 4: Deployment from Local Model\n",
    "\n",
    "This example shows how to deploy Qwen2.5-0.5B, a lightweight language model, from local model that we downloaded before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8db0e-63de-4e7d-8fea-711ae9668757",
   "metadata": {},
   "source": [
    "Check that we have the model files in the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484de7b-686e-47cc-a738-8c4a028f52d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /ephemeral/models/Qwen2.5-0.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676919b2-6c9f-4947-8cee-277d10419e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LOCAL_MODEL_DIR\"] = \"/ephemeral/models/Qwen2.5-0.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f63019-78b6-477a-8f03-486f7903f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm --name=\"Universal-LLM-NIM\" \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e NIM_MODEL_NAME=\"/opt/models/Qwen2.5-0.5B\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"Qwen/Qwen2.5-0.5B\" \\\n",
    " -v \"$LOCAL_MODEL_DIR:/opt/models/Qwen2.5-0.5B\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8001:8000 \\\n",
    " -d \\\n",
    " $IMG_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374acb3-06de-4905-8fb8-64c67a2ab2c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # For live logs\n",
    "# !docker logs -f $CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476b384-4b2e-4375-b008-d8ba825ff090",
   "metadata": {},
   "source": [
    "Below cell ensures that the LLM NIM is running before proceeding.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b>  LLM NIM service could take several miniutes to get ready.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2f781-8243-49e9-b317-3e1c6a01ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d14f8e-f8b3-42a9-92c6-9d861b8d6b70",
   "metadata": {},
   "source": [
    "Let's try out the LLM NIM service deployed with a local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ee9dd-083b-4db3-bef4-ed73de4813e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(model=\"Qwen/Qwen2.5-0.5B\",\n",
    "                       prompt=\"Once upon a time \")\n",
    "print(result['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42edf2ff-6882-471b-90f8-5777c566b074",
   "metadata": {},
   "source": [
    "Before we finish, let's stop the LLM NIM service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3457b28-9c0e-410a-b53e-9a9dc2ce0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
