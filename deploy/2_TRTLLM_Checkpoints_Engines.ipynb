{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab873001",
   "metadata": {},
   "source": [
    "<!--\n",
    "SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "-->\n",
    "\n",
    "# Deploy TensorRT-LLM Checkpoints and Engines with NIM\n",
    "\n",
    "This notebook demonstrates how to deploy your own TensorRT-LLM checkpoints and Engines with NIM. For\n",
    "demonstration purposes, we download and convert weights from HuggingFace manually, but note that you can also\n",
    "deploy HuggingFace weights directly without any manual conversion, as shown in [the previous notebook](./1_HuggingFace_Safetensors.ipynb).\n",
    "\n",
    "⚠️ This notebook assumes familiarity with TensorRT-LLM and TensorRT-LLM optimizations. Consider starting with Notebook 1\n",
    "unless you specifically need custom TensorRT optimizations.\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- ✅ Convert HuggingFace models to TensorRT-LLM format\n",
    "- ✅ Deploy TensorRT-LLM checkpoints for development\n",
    "- ✅ Compile highly optimized TensorRT-LLM engines\n",
    "- ✅ Deploy production-ready engines with maximum performance\n",
    "\n",
    "## When to Use This Approach\n",
    "\n",
    "**Choose this notebook if you:**\n",
    "- Need the absolute best inference performance\n",
    "- Have production workloads requiring low latency\n",
    "- Want to optimize for specific hardware configurations\n",
    "- Can invest time in the conversion process (15-45 minutes)\n",
    "\n",
    "## The Process\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[HuggingFace Model] --> B[TensorRT-LLM Checkpoint]\n",
    "    B --> C[TensorRT-LLM Engine]\n",
    "    C --> D[Deploy with NIM]\n",
    "\n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style D fill:#9f9,stroke:#333,stroke-width:2px\n",
    "```\n",
    "\n",
    "1. **Download**: Get a HuggingFace model (5 minutes)\n",
    "2. **Convert**: Create TensorRT-LLM checkpoint (10-15 minutes)\n",
    "3. **Compile**: Build optimized engine (10-30 minutes)\n",
    "4. **Deploy**: Serve with NIM (instant)\n",
    "\n",
    "## What's Covered\n",
    "\n",
    "This tutorial includes:\n",
    "* **Setup**: Preparing your environment and downloading models\n",
    "* **Example 1**: Converting Safetensors to TensorRT-LLM checkpoints\n",
    "* **Example 2**: Deploying checkpoints for testing\n",
    "* **Example 3**: Compiling optimized engines for production\n",
    "* **Example 4**: Deploying engines with performance benchmarking\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "TensorRT-LLM conversion and deployment requires significant resources:\n",
    "\n",
    "- **GPU**: NVIDIA GPU with at least 24GB VRAM (for Llama-3-8B)\n",
    "- **System Memory**: At least 64GB RAM recommended for conversion process\n",
    "- **Storage**:\n",
    "  - 50GB+ free space for model downloads and conversion artifacts\n",
    "  - SSD recommended for faster I/O during conversion\n",
    "\n",
    "**Conversion Time Estimates:**\n",
    "- Checkpoint conversion: 5-15 minutes depending on hardware\n",
    "- Engine compilation: 10-30 minutes depending on optimization settings\n",
    "\n",
    "For detailed hardware specifications, refer to the [TensorRT-LLM documentation](https://nvidia.github.io/TensorRT-LLM/).\n",
    "\n",
    "### System Setup\n",
    "\n",
    "First, let's verify your GPU setup and install necessary dependencies:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eec865a",
   "metadata": {},
   "source": [
    "### Install Required Software\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies for Docker management\n",
    "!python -m ensurepip --upgrade\n",
    "%pip install docker requests huggingface-hub && echo \"✓ Python dependencies installed successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add huggingface-cli PATH\n",
    "import os\n",
    "os.environ[\"PATH\"] = os.path.expanduser(\"~/.local/bin\") + \":\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd6932",
   "metadata": {},
   "source": [
    "### Get API Keys\n",
    "\n",
    "#### NVIDIA NGC API Key\n",
    "\n",
    "The NVIDIA NGC API Key is mandatory for accessing NVIDIA container registry and pulling secure container images.\n",
    "Refer to [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key) for more information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f7fef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    ngc_api_key = getpass.getpass(\"Enter your NGC API Key: \")\n",
    "    assert ngc_api_key.startswith(\"nvapi-\"), \"Not a valid key\"\n",
    "    os.environ[\"NGC_API_KEY\"] = ngc_api_key\n",
    "    print(\"✓ NGC API Key set successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6950de",
   "metadata": {},
   "source": [
    "#### Hugging Face Token\n",
    "\n",
    "You'll also need a [Huggingface Token](https://huggingface.co/settings/tokens) to download models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    hf_token = getpass.getpass(\"Enter your Huggingface Token: \")\n",
    "    assert hf_token.startswith(\"hf_\"), \"Not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    print(\"✓ Hugging Face token set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0beb0",
   "metadata": {},
   "source": [
    "### Setup NIM Container\n",
    "\n",
    "Choose your NIM container image and pull it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd22211",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set the NIM image\n",
    "os.environ['NIM_IMAGE'] = \"nvcr.io/nim/nvidia/llm-nim:latest\"\n",
    "print(f\"Using NIM image: {os.environ['NIM_IMAGE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ea0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the NIM container image\n",
    "!docker pull $NIM_IMAGE && echo \"✓ NIM container image pulled successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0c1e2",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Below are some utility functions we'll use in this notebook. These are for simplifying the process of deploying and monitoring NIMs in a notebook environment, and aren't required in general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834e81e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import docker\n",
    "import os\n",
    "\n",
    "def check_service_ready_from_logs(container_name, print_logs=False, timeout=600):\n",
    "    \"\"\"\n",
    "    Check if NIM service is ready by monitoring Docker logs for 'Application startup complete' message.\n",
    "\n",
    "    Args:\n",
    "        container_name (str): Name of the Docker container\n",
    "        print_logs (bool): Whether to print logs while monitoring (default: False)\n",
    "        timeout (int): Maximum time to wait in seconds (default: 600)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if service is ready, False if timeout reached\n",
    "    \"\"\"\n",
    "    print(\"Waiting for NIM service to start...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        client = docker.from_env()\n",
    "        container = client.containers.get(container_name)\n",
    "\n",
    "        # Stream logs in real-time using the blocking generator\n",
    "        log_buffer = \"\"\n",
    "        for log_chunk in container.logs(stdout=True, stderr=True, follow=True, stream=True):\n",
    "            # Check timeout\n",
    "            if time.time() - start_time > timeout:\n",
    "                print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "                return False\n",
    "\n",
    "            # Decode chunk and add to buffer\n",
    "            chunk = log_chunk.decode('utf-8', errors='ignore')\n",
    "            log_buffer += chunk\n",
    "\n",
    "            # Process complete lines\n",
    "            while '\\n' in log_buffer:\n",
    "                line, log_buffer = log_buffer.split('\\n', 1)\n",
    "                line = line.strip()\n",
    "\n",
    "                if print_logs and line:\n",
    "                    print(f\"[LOG] {line}\")\n",
    "\n",
    "                # Check for startup complete message\n",
    "                if \"Application startup complete\" in line:\n",
    "                    print(\"✓ Application startup complete! Service is ready.\")\n",
    "                    return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "    return False\n",
    "\n",
    "def check_service_ready():\n",
    "    \"\"\"Fallback health check using HTTP endpoint\"\"\"\n",
    "    url = 'http://localhost:8000/v1/health/ready'\n",
    "    print(\"Checking service health endpoint...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers={'accept': 'application/json'})\n",
    "            if response.status_code == 200 and response.json().get(\"message\") == \"Service is ready.\":\n",
    "                print(\"✓ Service ready!\")\n",
    "                break\n",
    "        except requests.ConnectionError:\n",
    "            pass\n",
    "        print(\"⏳ Still starting...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "def generate_text(model, prompt, max_tokens=1000, temperature=0.7):\n",
    "    \"\"\"Generate text using the NIM service\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"http://localhost:8000/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Utility functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71215820",
   "metadata": {},
   "source": [
    "### Download Base Model\n",
    "\n",
    "We'll download Llama-3-8B-Instruct as our base model for TensorRT-LLM conversion.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You can modify the `model_save_location` variable below to use a different directory for storing models and conversion artifacts.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea15aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base directory for all files - you can modify this path as needed\n",
    "# Examples: \".\", \"~\", \"/tmp\", \"/scratch\", etc.\n",
    "base_work_dir = \"/ephemeral\"\n",
    "os.environ[\"BASE_WORK_DIR\"] = base_work_dir\n",
    "\n",
    "# Set up model download location\n",
    "model_save_location = os.path.join(base_work_dir, \"models\")\n",
    "\n",
    "os.environ[\"MODEL_SAVE_LOCATION\"] = model_save_location\n",
    "os.environ[\"LOCAL_MODEL_DIR\"] = os.path.join(model_save_location, \"llama3-8b-instruct-hf\")\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs(os.environ[\"LOCAL_MODEL_DIR\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762720fe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> NVIDIA cannot guarantee the security of any models hosted on non-NVIDIA systems such as HuggingFace. Malicious or insecure models can result in serious security risks up to and including full remote code execution. We strongly recommend that before attempting to load it you manually verify the safety of any model not provided by NVIDIA, through such mechanisms as a) ensuring that the model weights are serialized using the safetensors format, b) conducting a manual review of any model or inference code to ensure that it is free of obfuscated or malicious code, and c) validating the signature of the model, if available, to ensure that it comes from a trusted source and has not been modified.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b> You must accept the model's license agreement at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct before using this model.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73363832",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hf download meta-llama/Meta-Llama-3-8B-Instruct --local-dir \"$LOCAL_MODEL_DIR\" && echo \"✓ Model downloaded successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb532d41",
   "metadata": {},
   "source": [
    "## TensorRT-LLM Conversion Examples\n",
    "\n",
    "Let's explore the complete workflow from Hugging Face models to optimized TensorRT-LLM engines.\n",
    "\n",
    "### Setup Common Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce749dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONTAINER_NAME\"] = \"TRTLLM-NIM\"\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = os.path.join(base_work_dir, \".cache/nim\")\n",
    "os.environ[\"TRTLLM_CKPT_DIR\"] = os.path.join(model_save_location, \"llama3-8b-instruct-ckpt\")\n",
    "os.environ[\"TRTLLM_ENGINE_DIR\"] = os.path.join(model_save_location, \"llama3-8b-instruct-engine\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.path.join(os.environ[\"TRTLLM_CKPT_DIR\"], \"trtllm_ckpt\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(os.environ[\"TRTLLM_ENGINE_DIR\"], \"trtllm_engine\"), exist_ok=True)\n",
    "\n",
    "print(\"✓ Directories created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a36ce1",
   "metadata": {},
   "source": [
    "## Example 1: Convert Safetensors to TensorRT-LLM Checkpoint\n",
    "\n",
    "First, we'll convert the Hugging Face safetensors model to a TensorRT-LLM checkpoint format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the source model files\n",
    "!ls -Rlh $LOCAL_MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac70039",
   "metadata": {},
   "source": [
    "### Convert to TensorRT-LLM Checkpoint\n",
    "\n",
    "We'll use the TensorRT-LLM tools inside the NIM container to perform the conversion.\n",
    "\n",
    "> INFO\n",
    "> For more information on TensorRT-LLM Checkpoints and the available options, refer to the [TensorRT-LLM Documentation](https://nvidia.github.io/TensorRT-LLM/architecture/checkpoint.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc8c7b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Convert safetensors to TensorRT-LLM checkpoint\n",
    "# This uses the checkpoint_convert.py script inside the NIM container\n",
    "print(\"Starting conversion to TensorRT-LLM checkpoint...\")\n",
    "print(\"This process may take a few minutes depending on your hardware.\")\n",
    "\n",
    "!docker run --rm \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus '\"device=0,1\"' \\\n",
    "  --shm-size=16GB \\\n",
    "  -v $LOCAL_MODEL_DIR:/input_model \\\n",
    "  -v $TRTLLM_CKPT_DIR:/output_dir \\\n",
    "  -u $(id -u) \\\n",
    "  $NIM_IMAGE \\\n",
    "  python3 app/tensorrt_llm/examples/models/core/llama/convert_checkpoint.py \\\n",
    "  --model_dir /input_model \\\n",
    "  --output_dir /output_dir/trtllm_ckpt \\\n",
    "  --dtype bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7ce1f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Copy the required files from the huggingface model directory to the TensorRT-LLM checkpoint directory\n",
    "!cp -r $LOCAL_MODEL_DIR/config.json $TRTLLM_CKPT_DIR/config.json\n",
    "!cp -r $LOCAL_MODEL_DIR/generation_config.json $TRTLLM_CKPT_DIR/generation_config.json\n",
    "!cp -r $LOCAL_MODEL_DIR/tokenizer.json $TRTLLM_CKPT_DIR/tokenizer.json\n",
    "!cp -r $LOCAL_MODEL_DIR/tokenizer_config.json $TRTLLM_CKPT_DIR/tokenizer_config.json\n",
    "!cp -r $LOCAL_MODEL_DIR/special_tokens_map.json $TRTLLM_CKPT_DIR/special_tokens_map.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the directory structure of the checkpoint folder\n",
    "!ls -Rlh $TRTLLM_CKPT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f92cc",
   "metadata": {},
   "source": [
    "## Example 2: Deploy TensorRT-LLM Checkpoint with NIM\n",
    "\n",
    "Now let's deploy the TensorRT-LLM checkpoint using NIM:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a273e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Deploy TensorRT-LLM checkpoint with NIM\n",
    "print(\"Deploying TensorRT-LLM checkpoint with NIM...\")\n",
    "\n",
    "!docker run -it --rm \\\n",
    "  --name=$CONTAINER_NAME \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus '\"device=0\"' \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NIM_MODEL_NAME=\"/opt/models/my_model\" \\\n",
    "  -e NIM_SERVED_MODEL_NAME=\"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n",
    "  -e NIM_MODEL_PROFILE=\"tensorrt_llm\" \\\n",
    "  -v \"$TRTLLM_CKPT_DIR:/opt/models/my_model\" \\\n",
    "  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  -d \\\n",
    "  $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6a271",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!docker ps  # Check container is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a60868",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17464d4",
   "metadata": {},
   "source": [
    "### Test TensorRT-LLM Checkpoint Deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6605b8c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Test the deployed TensorRT-LLM checkpoint\n",
    "result = generate_text(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    prompt=\"Explain the benefits of TensorRT-LLM optimization\"\n",
    ")\n",
    "print(\"TensorRT-LLM Checkpoint Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the checkpoint deployment before moving to engine compilation\n",
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b9e0d",
   "metadata": {},
   "source": [
    "## Example 3: Compile TensorRT-LLM Engine\n",
    "\n",
    "Now let's compile the checkpoint into a fully optimized TensorRT-LLM engine:\n",
    "\n",
    "For more information on TensorRT-LLM Engines and the available options, refer to the [trtllm-build documentation](https://nvidia.github.io/TensorRT-LLM/commands/trtllm-build.html).\n",
    "\n",
    "For detailed optimization guidance, refer to the [TensorRT-LLM Performance Guide](https://nvidia.github.io/TensorRT-LLM/performance/perf-overview.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31947d06",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Compile TensorRT-LLM checkpoint to engine\n",
    "print(\"Compiling TensorRT-LLM checkpoint to engine...\")\n",
    "print(\"This process may take several minutes depending on your hardware and optimization settings.\")\n",
    "\n",
    "!docker run --rm \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus all \\\n",
    "  --shm-size=16GB \\\n",
    "  -v $TRTLLM_CKPT_DIR:/input_checkpoints \\\n",
    "  -v $TRTLLM_ENGINE_DIR:/output_engines \\\n",
    "  -w /output_engines \\\n",
    "  -u $(id -u) \\\n",
    "  $NIM_IMAGE \\\n",
    "  trtllm-build --checkpoint_dir /input_checkpoints/trtllm_ckpt \\\n",
    "  --output_dir /output_engines/trtllm_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe9541",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Copy the required files from the huggingface model directory to the TensorRT-LLM engine directory\n",
    "!cp -r $LOCAL_MODEL_DIR/config.json $TRTLLM_ENGINE_DIR/config.json\n",
    "!cp -r $LOCAL_MODEL_DIR/generation_config.json $TRTLLM_ENGINE_DIR/generation_config.json\n",
    "!cp -r $LOCAL_MODEL_DIR/tokenizer.json $TRTLLM_ENGINE_DIR/tokenizer.json\n",
    "!cp -r $LOCAL_MODEL_DIR/tokenizer_config.json $TRTLLM_ENGINE_DIR/tokenizer_config.json\n",
    "!cp -r $LOCAL_MODEL_DIR/special_tokens_map.json $TRTLLM_ENGINE_DIR/special_tokens_map.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the engine was created\n",
    "!ls -Rlh $TRTLLM_ENGINE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0295c2b4",
   "metadata": {},
   "source": [
    "## Example 4: Deploy TensorRT-LLM Engine with NIM\n",
    "\n",
    "Finally, let's deploy the fully optimized TensorRT-LLM engine:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1146a2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Deploy TensorRT-LLM engine with NIM\n",
    "print(\"Deploying optimized TensorRT-LLM engine with NIM...\")\n",
    "\n",
    "!docker run -it --rm \\\n",
    "  --name=$CONTAINER_NAME \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus '\"device=0\"' \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NIM_MODEL_NAME=\"/opt/models/my_model\" \\\n",
    "  -e NIM_SERVED_MODEL_NAME=\"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n",
    "  -e NIM_MODEL_PROFILE=\"tensorrt_llm\" \\\n",
    "  -v $TRTLLM_ENGINE_DIR:/opt/models/my_model \\\n",
    "  -v $LOCAL_NIM_CACHE:/opt/nim/.cache \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  -d \\\n",
    "  $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ccf29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b56fca",
   "metadata": {},
   "source": [
    "### Test TensorRT-LLM Engine Deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the deployed TensorRT-LLM engine\n",
    "import time\n",
    "\n",
    "# Warm up the engine\n",
    "print(\"Warming up the TensorRT-LLM engine...\")\n",
    "generate_text(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    prompt=\"Hello\",\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "result = generate_text(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    prompt=\"Write a Python function to implement binary search\",\n",
    ")\n",
    "\n",
    "print(\"TensorRT-LLM Engine Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result if result else \"Failed to generate text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ae52e",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\"\n",
    "print(\"✓ Container stopped successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dc58f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete TensorRT-LLM workflow:\n",
    "\n",
    "1. **Checkpoint Conversion**: Converting Hugging Face safetensors to TensorRT-LLM checkpoint format\n",
    "2. **Checkpoint Deployment**: Deploying checkpoints with NIM for development and testing\n",
    "3. **Engine Compilation**: Creating optimized TensorRT-LLM engines for production\n",
    "4. **Engine Deployment**: Deploying optimized engines for maximum performance\n",
    "\n",
    "**Key Benefits of TensorRT-LLM:**\n",
    "- **Performance**: Up to 4x faster inference compared to standard frameworks\n",
    "- **Memory Efficiency**: Optimized memory usage and KV-cache management\n",
    "- **Flexibility**: Support for various optimization techniques and hardware configurations\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different optimization settings for your use case\n",
    "- Try quantization techniques (INT8, FP8) for further performance gains\n",
    "- Explore multi-GPU deployments for larger models\n",
    "\n",
    "For more advanced optimization techniques, refer to the [TensorRT-LLM documentation](https://nvidia.github.io/TensorRT-LLM/)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
