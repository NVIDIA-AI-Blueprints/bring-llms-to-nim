{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29a8d52",
   "metadata": {},
   "source": [
    "<!--\n",
    "SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "-->\n",
    "\n",
    "# Deploy GGUF Checkpoints with NIM\n",
    "\n",
    "This notebook shows you how to deploy memory-efficient quantized models using GGUF format with NVIDIA NIM. Perfect for running large models on consumer GPUs or maximizing the number of models per server.\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- ✅ Deploy quantized models that use 50-75% less memory\n",
    "- ✅ Run large models on consumer GPUs (8GB-16GB VRAM)\n",
    "- ✅ Choose the right quantization level for your needs\n",
    "- ✅ Handle GGUF's special configuration requirements\n",
    "\n",
    "## When to Use This Approach\n",
    "\n",
    "**Choose this notebook if you:**\n",
    "- Have limited GPU memory (8GB-16GB VRAM)\n",
    "- Want to run larger models on smaller GPUs\n",
    "- Need to deploy multiple models on one GPU\n",
    "- Can accept slight quality trade-offs for efficiency\n",
    "\n",
    "**Consider other notebooks if you:**\n",
    "- Have plenty of GPU memory (→ See Notebook 1: HuggingFace)\n",
    "- Need maximum quality/performance (→ See Notebook 2: TensorRT-LLM)\n",
    "\n",
    "## Understanding GGUF Quantization\n",
    "\n",
    "Quantization reduces model size by using fewer bits for weights:\n",
    "\n",
    "| Format | Model Size | Quality | Use Case |\n",
    "|--------|------------|---------|----------|\n",
    "| Full Precision | 100% (baseline) | Perfect | Research, fine-tuning |\n",
    "| Q8_0 | ~33% | Near-perfect | Quality-focused deployment |\n",
    "| Q5_K_M | ~22% | Excellent | Balanced deployment |\n",
    "| Q4_K_M | ~18% | Very Good | **Recommended** - best balance |\n",
    "| Q3_K_M | ~14% | Good | Memory-constrained |\n",
    "\n",
    "**Example**: Llama-3.2-3B\n",
    "- Full model: ~13GB → Won't fit on RTX 3060\n",
    "- Q4_K_M: ~2.1GB → Runs comfortably on 8GB GPUs\n",
    "\n",
    "## The GGUF Challenge\n",
    "\n",
    "GGUF files don't include configuration metadata, so we need to:\n",
    "1. Download the GGUF model file\n",
    "2. Get the config.json from the original model\n",
    "3. Organize them correctly for NIM\n",
    "\n",
    "Don't worry - we'll walk through this step-by-step!\n",
    "\n",
    "## What's Covered\n",
    "\n",
    "This tutorial includes:\n",
    "* **Setup**: Understanding GGUF requirements\n",
    "* **Example 1**: Deploying pre-downloaded GGUF models locally\n",
    "* **Example 2**: Comparing different quantization levels\n",
    "* **Example 3**: Custom deployment configurations\n",
    "* **Bonus**: Quick reference for all quantization options\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "GGUF deployment is more resource-friendly than full-precision models:\n",
    "\n",
    "- **GPU**: NVIDIA GPU with at least 8GB VRAM (for Llama-3.2-3B with Q4_K_M quantization)\n",
    "  - Recommended: RTX 4070, RTX 3080, or higher\n",
    "  - Supported: RTX 3060 12GB, RTX 4060 Ti 16GB for smaller models\n",
    "- **Driver**: NVIDIA Driver version 535 or higher\n",
    "- **CUDA**: CUDA 12.0 or higher\n",
    "- **System Memory**: At least 16GB RAM recommended\n",
    "- **Storage**: 5-15GB free space depending on quantization level\n",
    "\n",
    "**Model Size Estimates (Llama-3.2-3B):**\n",
    "- Q4_K_M: ~2.1GB (recommended balance of quality/size)\n",
    "- Q5_K_M: ~2.6GB (higher quality)\n",
    "- Q8_0: ~3.2GB (highest quality quantized)\n",
    "\n",
    "For detailed hardware specifications, refer to the [NIM LLM Documentation](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html).\n",
    "\n",
    "### System Setup\n",
    "\n",
    "First, let's verify your GPU setup and install necessary dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import docker\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f0b4a",
   "metadata": {},
   "source": [
    "### Install Required Software\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dce97f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Update system and install required packages\n",
    "!sudo apt-get update && echo \"✓ System packages updated successfully\"\n",
    "!sudo apt-get install git-lfs wget -y && echo \"✓ Git LFS and wget installed successfully\"\n",
    "!git lfs install && echo \"✓ Git LFS initialized successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "!pip install docker requests huggingface-hub && echo \"✓ Python dependencies installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9133551",
   "metadata": {},
   "source": [
    "### Get API Keys\n",
    "\n",
    "#### NVIDIA NGC API Key\n",
    "\n",
    "The NVIDIA NGC API Key is mandatory for accessing NVIDIA container registry and pulling secure container images.\n",
    "Refer to [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual NGC API key\n",
    "if not os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    ngc_api_key = getpass.getpass(\"Enter your NGC API Key: \")\n",
    "    assert ngc_api_key.startswith(\"nvapi-\"), \"Not a valid key\"\n",
    "    os.environ[\"NGC_API_KEY\"] = ngc_api_key\n",
    "    print(\"✓ NGC API Key set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b83087",
   "metadata": {},
   "source": [
    "### Docker Login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e384bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to NGC registry\n",
    "!echo \"$NGC_API_KEY\" | docker login nvcr.io -u '$oauthtoken' --password-stdin && echo \"✓ Docker login successful\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8371c44",
   "metadata": {},
   "source": [
    "#### Hugging Face Token\n",
    "\n",
    "You'll also need a [Huggingface Token](https://huggingface.co/settings/tokens) to download models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e09e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if not os.environ.get(\"HF_USERNAME\", \"\"):\n",
    "    hf_username = getpass.getpass(\"Enter your Huggingface Username: \")\n",
    "    os.environ[\"HF_USERNAME\"] = hf_username\n",
    "    print(\"✓ Hugging Face username set successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4019fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    hf_token = getpass.getpass(\"Enter your Huggingface Token: \")\n",
    "    assert hf_token.startswith(\"hf_\"), \"Not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    print(\"✓ Hugging Face token set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dfb06e",
   "metadata": {},
   "source": [
    "### Setup NIM Container\n",
    "\n",
    "Choose your NIM container image and pull it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736aeaa4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set the NIM image - using universal NIM for GGUF support\n",
    "os.environ['NIM_IMAGE'] = \"nvcr.io/nvidian/nim-llm-dev/universal-nim:1.11.0.rc6\"\n",
    "print(f\"Using NIM image: {os.environ['NIM_IMAGE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the NIM container image\n",
    "!docker pull $NIM_IMAGE && echo \"✓ NIM container image pulled successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e389760f",
   "metadata": {},
   "source": [
    "### Setup Common Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3acc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CONTAINER_NAME\"] = \"GGUF-NIM\"\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = os.path.expanduser(\"~/.cache/nim\")\n",
    "os.environ[\"GGUF_WORK_DIR\"] = os.path.expanduser(\"~/gguf_models\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"GGUF_WORK_DIR\"], exist_ok=True)\n",
    "\n",
    "print(\"✓ Directories created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148f170",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Below are some utility functions we'll use in this notebook. These are for simplifying the process of deploying and monitoring NIMs in a notebook environment, and aren't required in general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_service_ready_from_logs(container_name, print_logs=False, timeout=600):\n",
    "    \"\"\"\n",
    "    Check if NIM service is ready by monitoring Docker logs for 'Application startup complete' message.\n",
    "\n",
    "    Args:\n",
    "        container_name (str): Name of the Docker container\n",
    "        print_logs (bool): Whether to print logs while monitoring (default: False)\n",
    "        timeout (int): Maximum time to wait in seconds (default: 600)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if service is ready, False if timeout reached\n",
    "    \"\"\"\n",
    "    print(\"Waiting for NIM service to start...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        client = docker.from_env()\n",
    "        container = client.containers.get(container_name)\n",
    "\n",
    "        # Stream logs in real-time using the blocking generator\n",
    "        log_buffer = \"\"\n",
    "        for log_chunk in container.logs(stdout=True, stderr=True, follow=True, stream=True):\n",
    "            # Check timeout\n",
    "            if time.time() - start_time > timeout:\n",
    "                print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "                return False\n",
    "\n",
    "            # Decode chunk and add to buffer\n",
    "            chunk = log_chunk.decode('utf-8', errors='ignore')\n",
    "            log_buffer += chunk\n",
    "\n",
    "            # Process complete lines\n",
    "            while '\\n' in log_buffer:\n",
    "                line, log_buffer = log_buffer.split('\\n', 1)\n",
    "                line = line.strip()\n",
    "\n",
    "                if print_logs and line:\n",
    "                    print(f\"[LOG] {line}\")\n",
    "\n",
    "                # Check for startup complete message\n",
    "                if \"Application startup complete\" in line:\n",
    "                    print(\"✓ Application startup complete! Service is ready.\")\n",
    "                    return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "    return False\n",
    "\n",
    "def check_service_ready():\n",
    "    \"\"\"Fallback health check using HTTP endpoint\"\"\"\n",
    "    url = 'http://localhost:8000/v1/health/ready'\n",
    "    print(\"Checking service health endpoint...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers={'accept': 'application/json'})\n",
    "            if response.status_code == 200 and response.json().get(\"message\") == \"Service is ready.\":\n",
    "                print(\"✓ Service ready!\")\n",
    "                break\n",
    "        except requests.ConnectionError:\n",
    "            pass\n",
    "        print(\"⏳ Still starting...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "def generate_text(model, prompt, max_tokens=250, temperature=0.7):\n",
    "    \"\"\"Generate text using the NIM service\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"http://localhost:8000/v1/completions\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Utility functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb84e46",
   "metadata": {},
   "source": [
    "## GGUF Deployment Examples\n",
    "\n",
    "Let's explore how to deploy GGUF models locally using NIM.\n",
    "\n",
    "## Example 1: Pre-download and Local GGUF Deployment\n",
    "\n",
    "This example shows how to pre-download GGUF models and deploy them locally. This approach provides reliable offline usage and faster startup times since models are already available locally.\n",
    "\n",
    "### Download External Config File\n",
    "\n",
    "GGUF repositories don't include the config.json file needed by NIM. We need to download it from the original Llama-3.2-3B-Instruct repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d770e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create a temporary directory for the config file\n",
    "config_temp_dir = os.path.expanduser(\"~/gguf_config_temp\")\n",
    "os.makedirs(config_temp_dir, exist_ok=True)\n",
    "os.environ[\"CONFIG_TEMP_DIR\"] = config_temp_dir\n",
    "\n",
    "# Download config.json from the original Llama-3.2-3B-Instruct repository\n",
    "print(\"Downloading config.json from original model repository...\")\n",
    "!wget -O \"$CONFIG_TEMP_DIR/config.json\" https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json && echo \"✓ Config file downloaded successfully\"\n",
    "\n",
    "# Also download tokenizer files that may be needed\n",
    "!wget -O \"$CONFIG_TEMP_DIR/tokenizer.json\" https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/tokenizer.json 2>/dev/null || echo \"tokenizer.json not found - continuing\"\n",
    "!wget -O \"$CONFIG_TEMP_DIR/tokenizer_config.json\" https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/tokenizer_config.json 2>/dev/null || echo \"tokenizer_config.json not found - continuing\"\n",
    "\n",
    "print(\"✓ Configuration files downloaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cced832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the config file was downloaded\n",
    "!ls -la \"$CONFIG_TEMP_DIR\"\n",
    "!head -5 \"$CONFIG_TEMP_DIR/config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3995822",
   "metadata": {},
   "source": [
    "### Download GGUF Model Locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87cbc59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create separate directories for different quantizations\n",
    "q4_model_path = os.path.join(os.environ[\"GGUF_WORK_DIR\"], \"Llama-3.2-3B-Instruct-Q4_K_M\")\n",
    "q8_model_path = os.path.join(os.environ[\"GGUF_WORK_DIR\"], \"Llama-3.2-3B-Instruct-Q8_0\")\n",
    "\n",
    "os.makedirs(q4_model_path, exist_ok=True)\n",
    "os.makedirs(q8_model_path, exist_ok=True)\n",
    "\n",
    "os.environ[\"Q4_MODEL_PATH\"] = q4_model_path\n",
    "os.environ[\"Q8_MODEL_PATH\"] = q8_model_path\n",
    "\n",
    "# Download specific GGUF model files to their respective directories\n",
    "print(\"Downloading GGUF model files locally...\")\n",
    "print(\"This may take several minutes depending on your internet connection...\")\n",
    "\n",
    "# Download the Q4_K_M quantization\n",
    "!wget -O \"$Q4_MODEL_PATH/Llama-3.2-3B-Instruct-Q4_K_M.gguf\" \\\n",
    "  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf && \\\n",
    "  echo \"✓ Q4_K_M quantization downloaded successfully\"\n",
    "\n",
    "# Download the Q8_0 quantization\n",
    "!wget -O \"$Q8_MODEL_PATH/Llama-3.2-3B-Instruct-Q8_0.gguf\" \\\n",
    "  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf && \\\n",
    "  echo \"✓ Q8_0 quantization downloaded successfully\"\n",
    "\n",
    "print(\"✓ GGUF model files downloaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258947a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Verify the download and check available quantization files\n",
    "!ls -la \"$Q4_MODEL_PATH\"/*.gguf\n",
    "!ls -la \"$Q8_MODEL_PATH\"/*.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664505d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Copy configuration files to both quantization directories\n",
    "!cp \"$CONFIG_TEMP_DIR/config.json\" \"$Q4_MODEL_PATH/\" && echo \"✓ Config file copied to Q4_K_M directory\"\n",
    "!cp \"$CONFIG_TEMP_DIR/tokenizer\"*.json \"$Q4_MODEL_PATH/\" 2>/dev/null || echo \"Some tokenizer files not found - continuing\"\n",
    "\n",
    "!cp \"$CONFIG_TEMP_DIR/config.json\" \"$Q8_MODEL_PATH/\" && echo \"✓ Config file copied to Q8_0 directory\"\n",
    "!cp \"$CONFIG_TEMP_DIR/tokenizer\"*.json \"$Q8_MODEL_PATH/\" 2>/dev/null || echo \"Some tokenizer files not found - continuing\"\n",
    "\n",
    "print(\"✓ Configuration files copied to all quantization directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the complete model setup for both quantizations\n",
    "!echo \"Q4_K_M model directory:\"\n",
    "!ls -la \"$Q4_MODEL_PATH\"\n",
    "!echo\n",
    "!echo \"Q8_0 model directory:\"\n",
    "!ls -la \"$Q8_MODEL_PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4b27d",
   "metadata": {},
   "source": [
    "### Deploy Local GGUF Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a6aad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Deploy Q4_K_M quantization locally\n",
    "print(\"Deploying Q4_K_M quantization locally...\")\n",
    "\n",
    "!docker run -it --rm \\\n",
    "  --name=$CONTAINER_NAME \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus all \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NIM_MODEL_NAME=\"/opt/models/q4_model\" \\\n",
    "  -e NIM_SERVED_MODEL_NAME=\"meta-llama/Llama-3.2-3B-Instruct\" \\\n",
    "  -v \"$Q4_MODEL_PATH:/opt/models/q4_model\" \\\n",
    "  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  -d \\\n",
    "  $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "if not check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True):\n",
    "    print(\"Falling back to health endpoint check...\")\n",
    "    check_service_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48cbf69",
   "metadata": {},
   "source": [
    "### Test Local GGUF Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5d437",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Test the Q4_K_M model deployment\n",
    "result = generate_text(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    prompt=\"Explain the concept of machine learning in simple terms:\",\n",
    "    max_tokens=200\n",
    ")\n",
    "print(\"Q4_K_M Quantization Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result['choices'][0]['text'] if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81197123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the current deployment\n",
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d04bbe",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Available Quantization Levels\n",
    "\n",
    "The bartowski/Llama-3.2-3B-Instruct-GGUF repository includes multiple quantization levels. Each quantization requires its own directory with the GGUF file and configuration files.\n",
    "\n",
    "### Directory Structure for Each Quantization:\n",
    "\n",
    "Each quantization needs to be organized as follows:\n",
    "```\n",
    "quantization_directory/\n",
    "├── config.json                    # From original model repo\n",
    "├── tokenizer.json                 # From original model repo\n",
    "├── tokenizer_config.json          # From original model repo\n",
    "└── model_name-QUANTIZATION.gguf   # The quantized model file\n",
    "```\n",
    "\n",
    "### Downloaded Quantizations:\n",
    "\n",
    "**Q4_K_M (Recommended - Best Balance)**\n",
    "- Directory: `$Q4_MODEL_PATH`\n",
    "- Size: ~2.1GB\n",
    "- Quality: Good balance of quality and efficiency\n",
    "- Memory: ~4GB VRAM required\n",
    "\n",
    "**Q8_0 (Highest Quality Quantized)**\n",
    "- Directory: `$Q8_MODEL_PATH`\n",
    "- Size: ~3.2GB\n",
    "- Quality: Near full-precision quality\n",
    "- Memory: ~6GB VRAM required\n",
    "\n",
    "### Additional Quantizations Available:\n",
    "\n",
    "**Q5_K_M (Higher Quality)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a7662",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Create directory and download\n",
    "Q5_MODEL_PATH=\"$GGUF_WORK_DIR/Llama-3.2-3B-Instruct-Q5_K_M\"\n",
    "!mkdir -p \"$Q5_MODEL_PATH\"\n",
    "!wget -O \"$Q5_MODEL_PATH/Llama-3.2-3B-Instruct-Q5_K_M.gguf\" \\\n",
    "  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf\n",
    "\n",
    "# Copy configuration files\n",
    "!cp \"$CONFIG_TEMP_DIR\"/*.json \"$Q5_MODEL_PATH/\"\n",
    "\n",
    "# Deploy with:\n",
    "-e NIM_MODEL_NAME=\"/opt/models/q5_model\" \\\n",
    "-v \"$Q5_MODEL_PATH:/opt/models/q5_model\" \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33af6fc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- Size: ~2.6GB\n",
    "- Quality: Better quality than Q4_K_M\n",
    "- Memory: ~5GB VRAM required\n",
    "\n",
    "**Other Available Quantizations:**\n",
    "- Q2_K: Ultra-compressed (~1.3GB)\n",
    "- Q3_K_M: Small size (~1.7GB)\n",
    "- Q6_K: High quality (~2.9GB)\n",
    "- IQ4_XS: Experimental quantization\n",
    "\n",
    "To set up any additional quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95914db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for the quantization\n",
    "QUANT_DIR=\"$GGUF_WORK_DIR/Model-QUANTIZATION_NAME\"\n",
    "!mkdir -p \"$QUANT_DIR\"\n",
    "\n",
    "# Download the GGUF file\n",
    "!wget -O \"$QUANT_DIR/FILENAME.gguf\" \\\n",
    "  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/FILENAME.gguf\n",
    "\n",
    "# Copy configuration files\n",
    "!cp \"$CONFIG_TEMP_DIR\"/*.json \"$QUANT_DIR/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38b4c5",
   "metadata": {},
   "source": [
    "## Example 2: Performance Comparison with Different Quantization Levels\n",
    "\n",
    "Let's deploy different quantizations to understand the trade-offs. Since each quantization needs its own directory, we'll deploy them separately:\n",
    "\n",
    "### Deploy Q8_0 Quantization (Higher Quality)\n",
    "\n",
    "Each GGUF quantization must be in its own directory containing the GGUF file and configuration files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e7786",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Deploy Q8_0 quantization for comparison\n",
    "print(\"Deploying Q8_0 quantization (higher quality, larger size)...\")\n",
    "\n",
    "!docker run -it --rm \\\n",
    "  --name=$CONTAINER_NAME \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus all \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NIM_MODEL_NAME=\"/opt/models/q8_model\" \\\n",
    "  -e NIM_SERVED_MODEL_NAME=\"meta-llama/Llama-3.2-3B-Instruct-Q8\" \\\n",
    "  -v \"$Q8_MODEL_PATH:/opt/models/q8_model\" \\\n",
    "  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  -d \\\n",
    "  $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66be0f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Use the log-based check\n",
    "if not check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True):\n",
    "    print(\"Falling back to health endpoint check...\")\n",
    "    check_service_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da85e27",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Test Q8_0 quantization\n",
    "result = generate_text(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct-Q8\",\n",
    "    prompt=\"Write a brief story about a robot learning to paint:\",\n",
    "    max_tokens=150\n",
    ")\n",
    "print(\"Q8_0 Quantization Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result['choices'][0]['text'] if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2302244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the current deployment\n",
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2da078",
   "metadata": {},
   "source": [
    "## Example 3: Custom Configuration Deployment\n",
    "\n",
    "This example shows how to deploy with custom NIM parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a96be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Deploy Q4_K_M with custom parameters\n",
    "print(\"Deploying with custom configuration...\")\n",
    "\n",
    "!docker run -it --rm \\\n",
    "  --name=$CONTAINER_NAME \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus all \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NIM_MODEL_NAME=\"/opt/models/q4_model\" \\\n",
    "  -e NIM_SERVED_MODEL_NAME=\"meta-llama/Llama-3.2-3B-Instruct\" \\\n",
    "  -e NIM_MAX_INPUT_LENGTH=4096 \\\n",
    "  -e NIM_MAX_OUTPUT_LENGTH=1024 \\\n",
    "  -e NIM_MODEL_PROFILE=\"vllm\" \\\n",
    "  -v \"$Q4_MODEL_PATH:/opt/models/q4_model\" \\\n",
    "  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  -d \\\n",
    "  $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c1062",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Use the log-based check\n",
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with longer input/output capabilities\n",
    "result = generate_text(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    prompt=\"Create a detailed plan for building a web application using modern technologies. Include frontend, backend, database, and deployment considerations:\",\n",
    "    max_tokens=500\n",
    ")\n",
    "print(\"Custom Configuration Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result['choices'][0]['text'] if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05736931",
   "metadata": {},
   "source": [
    "## Performance Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f661c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison test\n",
    "test_prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Write a simple Python loop:\",\n",
    "    \"Explain quantum computing:\",\n",
    "    \"Create a recipe for chocolate cake:\"\n",
    "]\n",
    "\n",
    "print(\"Performance testing with GGUF Q4_K_M:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    start_time = time.time()\n",
    "    result = generate_text(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Test {i}: {end_time - start_time:.2f}s - {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4df08",
   "metadata": {},
   "source": [
    "## Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93acbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\"\n",
    "print(\"✓ Container stopped successfully\")\n",
    "\n",
    "# Optional: Clean up downloaded models (uncomment if you want to save disk space)\n",
    "# !rm -rf \"$GGUF_WORK_DIR\"\n",
    "# !rm -rf \"$CONFIG_TEMP_DIR\"\n",
    "# print(\"✓ Downloaded models cleaned up\")\n",
    "\n",
    "print(\"✓ All containers stopped successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e88b9b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated deploying GGUF checkpoints with NIM:\n",
    "\n",
    "1. **Local Deployment**: Pre-downloading models with separate directories per quantization\n",
    "2. **Quantization Options**: Understanding different quantization levels and their trade-offs\n",
    "3. **Custom Configuration**: Deploying with custom NIM parameters\n",
    "\n",
    "**Key Points:**\n",
    "- GGUF models require external config.json files from the original model repository\n",
    "- Each quantization level must be in its own directory with the GGUF file and configuration files\n",
    "- Different quantization levels offer trade-offs between model size, quality, and memory usage\n",
    "- Q4_K_M provides the best balance for most use cases\n",
    "- Local deployment enables offline usage and faster startup times\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different quantization levels for your specific use case\n",
    "- Try other GGUF models from the community\n",
    "- Compare performance across different quantizations\n",
    "- Set up additional quantization directories as needed\n",
    "\n",
    "For more information about GGUF format and quantization techniques, refer to the community documentation and model cards on Hugging Face."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
