{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51206717",
   "metadata": {},
   "source": [
    "<!--\n",
    "SPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "-->\n",
    "\n",
    "# Deploy HuggingFace Safetensor LLM Weights with NIM\n",
    "\n",
    "This notebook shows you how to deploy the most common type of LLM - HuggingFace models in Safetensors format - using NVIDIA NIM. This is the easiest way to get started with NIM deployment. For the complete list of LLMs supported by NIM, see the [NIM LLM Documentation](https://docs.nvidia.com/nim/large-language-models/latest/supported-llm-agnostic-architectures.html)\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- ✅ Deploy any HuggingFace model with a single Docker command\n",
    "- ✅ Switch between inference backends (TensorRT-LLM, vLLM) for different use cases\n",
    "- ✅ Customize deployment parameters for your specific needs\n",
    "- ✅ Deploy models offline from local storage\n",
    "\n",
    "## When to Use This Approach\n",
    "\n",
    "**Choose this notebook if you:**\n",
    "- Want to deploy popular models like Llama, Mistral, or Codestral\n",
    "- Have models already available on HuggingFace\n",
    "- Need a quick deployment without manual optimization\n",
    "- Want to experiment with different inference backends\n",
    "\n",
    "## How It Works\n",
    "\n",
    "NIM makes deployment simple:\n",
    "1. **You provide**: A HuggingFace model path (e.g., `mistralai/Codestral-22B-v0.1`)\n",
    "2. **NIM automatically**:\n",
    "   - Downloads the model\n",
    "   - Analyzes the architecture\n",
    "   - Selects the best backend\n",
    "   - Optimizes for your GPU\n",
    "   - Starts serving with an OpenAI-compatible API\n",
    "\n",
    "## What's Covered\n",
    "\n",
    "This tutorial includes:\n",
    "* **Example 1**: Basic deployment from HuggingFace (5 minutes)\n",
    "* **Example 2**: Choosing different backends (TensorRT-LLM vs vLLM)\n",
    "* **Example 3**: Customizing model parameters\n",
    "* **Example 4**: Deploying from local storage for offline use\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "Before proceeding, ensure your system meets the following requirements:\n",
    "\n",
    "- **GPU**: NVIDIA GPU with at least 32GB VRAM (for Codestral-22B) or 8GB VRAM (for smaller models like Qwen2.5-0.5B)\n",
    "- **System Memory**: At least 32GB RAM recommended\n",
    "- **Storage**: Sufficient disk space for model downloads and caching\n",
    "\n",
    "For detailed hardware specifications, please refer to the [NIM LLM Documentation](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html).\n",
    "\n",
    "### System Setup\n",
    "\n",
    "First, let's verify your GPU setup and install necessary dependencies:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b463ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c39db7",
   "metadata": {},
   "source": [
    "### Install Required Software\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de15737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "%pip install docker requests huggingface-hub && echo \"✓ Python dependencies installed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b181f",
   "metadata": {},
   "source": [
    "### Get API Keys\n",
    "\n",
    "#### NVIDIA NGC API Key\n",
    "\n",
    "The NVIDIA NGC API Key is mandatory for accessing NVIDIA container registry and pulling secure container images.\n",
    "Refer to [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-api-key) for more information.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c144c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    ngc_api_key = getpass.getpass(\"Enter your NGC API Key: \")\n",
    "    assert ngc_api_key.startswith(\"nvapi-\"), \"Not a valid key\"\n",
    "    os.environ[\"NGC_API_KEY\"] = ngc_api_key\n",
    "    print(\"✓ NGC API Key set successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7823d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4d449",
   "metadata": {},
   "source": [
    "#### Hugging Face Token\n",
    "\n",
    "You'll also need a [Huggingface Token](https://huggingface.co/settings/tokens) to download models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb839e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    hf_token = getpass.getpass(\"Enter your Huggingface Token: \")\n",
    "    assert hf_token.startswith(\"hf_\"), \"Not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    print(\"✓ Hugging Face token set successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecdf9ae",
   "metadata": {},
   "source": [
    "### Setup NIM Container\n",
    "\n",
    "Choose your NIM container image and pull it:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a09041",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set the NIM image\n",
    "os.environ['NIM_IMAGE'] = \"nvcr.io/nvidia/nim/nim-llm:1.11.0\"\n",
    "print(f\"Using NIM image: {os.environ['NIM_IMAGE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbabf298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the NIM container image\n",
    "!docker pull $NIM_IMAGE && echo \"✓ NIM container image pulled successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707e495",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "Below are some utility functions we'll use in this notebook. These are for simplifying the process of deploying and monitoring NIMs in a notebook environment, and aren't required in general.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4db25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import docker\n",
    "import os\n",
    "\n",
    "def check_service_ready_from_logs(container_name, print_logs=False, timeout=600):\n",
    "    \"\"\"\n",
    "    Check if NIM service is ready by monitoring Docker logs for 'Application startup complete' message.\n",
    "\n",
    "    Args:\n",
    "        container_name (str): Name of the Docker container\n",
    "        print_logs (bool): Whether to print logs while monitoring (default: False)\n",
    "        timeout (int): Maximum time to wait in seconds (default: 600)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if service is ready, False if timeout reached\n",
    "    \"\"\"\n",
    "    print(\"Waiting for NIM service to start...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        client = docker.from_env()\n",
    "        container = client.containers.get(container_name)\n",
    "\n",
    "        # Stream logs in real-time using the blocking generator\n",
    "        log_buffer = \"\"\n",
    "        for log_chunk in container.logs(stdout=True, stderr=True, follow=True, stream=True):\n",
    "            # Check timeout\n",
    "            if time.time() - start_time > timeout:\n",
    "                print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "                return False\n",
    "\n",
    "            # Decode chunk and add to buffer\n",
    "            chunk = log_chunk.decode('utf-8', errors='ignore')\n",
    "            log_buffer += chunk\n",
    "\n",
    "            # Process complete lines\n",
    "            while '\\n' in log_buffer:\n",
    "                line, log_buffer = log_buffer.split('\\n', 1)\n",
    "                line = line.strip()\n",
    "\n",
    "                if print_logs and line:\n",
    "                    print(f\"[LOG] {line}\")\n",
    "\n",
    "                # Check for startup complete message\n",
    "                if \"Application startup complete\" in line:\n",
    "                    print(\"✓ Application startup complete! Service is ready.\")\n",
    "                    return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"❌ Timeout reached ({timeout}s). Service may not have started properly.\")\n",
    "    return False\n",
    "\n",
    "def check_service_ready():\n",
    "    \"\"\"Fallback health check using HTTP endpoint\"\"\"\n",
    "    url = 'http://localhost:8000/v1/health/ready'\n",
    "    print(\"Checking service health endpoint...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, headers={'accept': 'application/json'})\n",
    "            if response.status_code == 200 and response.json().get(\"message\") == \"Service is ready.\":\n",
    "                print(\"✓ Service ready!\")\n",
    "                break\n",
    "        except requests.ConnectionError:\n",
    "            pass\n",
    "        print(\"⏳ Still starting...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "def generate_text(model, prompt, max_tokens=1000, temperature=0.7):\n",
    "    \"\"\"Generate text using the NIM service\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"http://localhost:8000/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()['choices'][0]['message']['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Utility functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b24b1",
   "metadata": {},
   "source": [
    "## Deployment Examples\n",
    "\n",
    "Let's explore different ways to deploy models using NIM.\n",
    "\n",
    "### Example 1: Basic Deployment from Hugging Face\n",
    "\n",
    "This example shows how to deploy Codestral-22B with default settings directly from Hugging Face.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> NVIDIA cannot guarantee the security of any models hosted on non-NVIDIA systems such as HuggingFace. Malicious or insecure models can result in serious security risks up to and including full remote code execution. We strongly recommend that before attempting to load it you manually verify the safety of any model not provided by NVIDIA, through such mechanisms as a) ensuring that the model weights are serialized using the safetensors format, b) conducting a manual review of any model or inference code to ensure that it is free of obfuscated or malicious code, and c) validating the signature of the model, if available, to ensure that it comes from a trusted source and has not been modified.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b> You must accept the model's license agreement at https://huggingface.co/mistralai/Codestral-22B-v0.1 before using this model.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27ea0a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set base directory for all files - you can modify this path as needed\n",
    "# Examples: \".\", \"~\", \"/tmp\", \"/scratch\", etc.\n",
    "base_work_dir = \"/ephemeral\"\n",
    "os.environ[\"BASE_WORK_DIR\"] = base_work_dir\n",
    "\n",
    "container_name = \"LLM-NIM\"\n",
    "os.environ[\"CONTAINER_NAME\"] = container_name\n",
    "os.environ[\"LOCAL_NIM_CACHE\"] = os.path.join(base_work_dir, \".cache/nim\")\n",
    "os.makedirs(os.environ[\"LOCAL_NIM_CACHE\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271bbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee24ba2",
   "metadata": {},
   "source": [
    "After running the following cell, you should be able to see the `LLM-NIM` container running.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps  # Check container is running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c64765",
   "metadata": {},
   "source": [
    "While the LLM NIM service is getting ready, you may run the following cell to see live logs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> NIM service takes a few minutes to initialize. Monitor with logs if needed.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbdaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392df9b0",
   "metadata": {},
   "source": [
    "Now let's test the deployed model:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\n",
    "    model=\"mistralai/Codestral-22B-v0.1\",\n",
    "    prompt=\"Write a complete function that computes fibonacci numbers in Rust\"\n",
    ")\n",
    "print(result if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb902390",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fd1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3ab77",
   "metadata": {},
   "source": [
    "### Example 2: Deployment Using Different Backend Options\n",
    "\n",
    "NIM supports multiple backends for model deployment. Let's explore TensorRT-LLM and vLLM backends:\n",
    "\n",
    "#### TensorRT-LLM Backend\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759f234",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Using TensorRT-LLM backend by specifying the NIM_MODEL_PROFILE parameter\n",
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_MODEL_PROFILE=\"tensorrt_llm\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b3734",
   "metadata": {},
   "source": [
    "Test the TensorRT-LLM backend:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\n",
    "    model=\"mistralai/Codestral-22B-v0.1\",\n",
    "    prompt=\"Write a complete Python function that computes fibonacci numbers with memoization\"\n",
    ")\n",
    "print(\"TensorRT-LLM Backend Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c25ffbd",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc65b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e5f0a",
   "metadata": {},
   "source": [
    "#### vLLM Backend\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ce511",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Using vLLM backend by specifying the NIM_MODEL_PROFILE parameter\n",
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_MODEL_PROFILE=\"vllm\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b9643",
   "metadata": {},
   "source": [
    "Test the vLLM backend:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dbba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(\n",
    "    model=\"mistralai/Codestral-22B-v0.1\",\n",
    "    prompt=\"Write a complete C++ function that computes fibonacci numbers efficiently\"\n",
    ")\n",
    "print(\"vLLM Backend Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(result if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024eba7",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fed5f0",
   "metadata": {},
   "source": [
    "### Example 3: Customizing Model Parameters\n",
    "\n",
    "This example demonstrates how custom parameters affect model behavior. We'll deploy with specific constraints and test them:\n",
    "\n",
    "**Key Parameters:**\n",
    "* `NIM_TENSOR_PARALLEL_SIZE=2`: Uses 2 GPUs in parallel for better performance\n",
    "* `NIM_MAX_INPUT_LENGTH=2048`: Limits input to 2048 tokens\n",
    "* `NIM_MAX_OUTPUT_LENGTH=512`: Limits output to 512 tokens\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> You must have at least 2 GPUs to run the following cell. If you don't have at least 2 GPUs, modify the <code>NIM_TENSOR_PARALLEL_SIZE</code> paramater in the cell below.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572e72f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus all \\\n",
    " --shm-size=16GB \\\n",
    " -e HF_TOKEN=$HF_TOKEN \\\n",
    " -e NIM_MODEL_NAME=\"hf://mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"mistralai/Codestral-22B-v0.1\" \\\n",
    " -e NIM_TENSOR_PARALLEL_SIZE=2 \\\n",
    " -e NIM_MAX_INPUT_LENGTH=2048 \\\n",
    " -e NIM_MAX_OUTPUT_LENGTH=512 \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e596290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8335c",
   "metadata": {},
   "source": [
    "Test with custom parameters:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc931c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_text(model=\"mistralai/Codestral-22B-v0.1\",\n",
    "                       prompt=\"Write me a function that computes fibonacci in Javascript\")\n",
    "print(result if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6f477",
   "metadata": {},
   "source": [
    "Before we move onto the next example, let's stop the LLM NIM service.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f0713",
   "metadata": {},
   "source": [
    "### Example 4: Deployment from Local Model\n",
    "\n",
    "This example shows how to deploy Qwen2.5-0.5B from the locally downloaded model:\n",
    "\n",
    "#### Download Model to Local Storage\n",
    "\n",
    "We'll download Qwen2.5-0.5B, a lightweight LLM, for use in Example 4.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You can modify the `model_save_location` variable below to use a different directory for storing downloaded models.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c9916",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set up local model directory\n",
    "model_save_location = os.path.join(base_work_dir, \"models\")\n",
    "local_model_name = \"Qwen2.5-0.5B-Instruct\"\n",
    "local_model_path = os.path.join(model_save_location, local_model_name)\n",
    "os.makedirs(local_model_path, exist_ok=True)\n",
    "\n",
    "os.environ[\"LOCAL_MODEL_DIR\"] = local_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e7351",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct --local-dir \"$LOCAL_MODEL_DIR\" && echo \"✓ Model downloaded successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0142da7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Verify model files exist\n",
    "!ls -Rlh \"$LOCAL_MODEL_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07d6f8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!docker run -it --rm \\\n",
    " --name=$CONTAINER_NAME \\\n",
    " --runtime=nvidia \\\n",
    " --gpus '\"device=0\"' \\\n",
    " --shm-size=16GB \\\n",
    " -e NIM_MODEL_NAME=\"/opt/models/local_model\" \\\n",
    " -e NIM_SERVED_MODEL_NAME=\"Qwen/Qwen2.5-0.5B\" \\\n",
    " -v \"$LOCAL_MODEL_DIR:/opt/models/local_model\" \\\n",
    " -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    " -u $(id -u) \\\n",
    " -p 8000:8000 \\\n",
    " -d \\\n",
    " $NIM_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the log-based check (set print_logs=True to see detailed logs)\n",
    "check_service_ready_from_logs(os.environ[\"CONTAINER_NAME\"], print_logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3808a1b",
   "metadata": {},
   "source": [
    "Test the local model deployment:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228a12a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "result = generate_text(model=\"Qwen/Qwen2.5-0.5B\",\n",
    "                       prompt=\"Tell me a story about a cat\")\n",
    "print(result if result else \"Failed to generate text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb5365",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "!docker stop $CONTAINER_NAME 2>/dev/null || echo \"Container already stopped\"\n",
    "print(\"✓ All containers stopped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b169f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
